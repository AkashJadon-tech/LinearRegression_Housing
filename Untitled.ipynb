{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e18a30-4677-4bc3-9d5c-a1c6c73ea60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Most of the code is copied from eng_fra_seq2seq_basic_v1 just modified for attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68f70f5d-bd9e-4cdf-a959-29b5b2610b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "from tensorflow.keras.layers import Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ffa150-c04e-484e-911f-c724ceecb2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch_size, hidden_size)\n",
    "        # values shape: (batch_size, seq_len, hidden_size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1242d256-a0ed-49e2-81f6-3bae468f09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConcatLayer(Layer):\n",
    "    def __init__(self, lstm_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = BahdanauAttention(lstm_units) \n",
    "    def call(self, decoder_outputs, encoder_outputs):\n",
    "        #decoder_outputs(batch, tgt_seq_len, lstm_units)\n",
    "        def apply_attention(dec_t):\n",
    "            # dec_t-shape (batch, lstm_units) (for single time step)\n",
    "            context_vector, _ = self.attention(dec_t, encoder_outputs)\n",
    "            return context_vector\n",
    "        #swap to time-major for map_fn(tgt_seq_len, batch, lstm_units)\n",
    "        decoder_outputs_time_major = tf.transpose(decoder_outputs, [1, 0, 2])\n",
    "        #compute context vector for each decoder timestep (returns: (tgt_seq_len, batch, lstm_units))\n",
    "        context_seq_time_major = tf.map_fn(\n",
    "            lambda dec_t: apply_attention(dec_t), \n",
    "            decoder_outputs_time_major, \n",
    "            fn_output_signature=tf.float32\n",
    "        )\n",
    "        #return to batch-major(batch, tgt_seq_len, lstm_units)\n",
    "        context_sequence = tf.transpose(context_seq_time_major, [1, 0, 2])\n",
    "        combined = Concatenate(axis=-1)([decoder_outputs, context_sequence])\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b556521b-e37b-4cc9-8e8f-e411584472d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load our data by giving path as well as max. no of samples\n",
    "def load_data(path , num_samples=10000):\n",
    "    #open the txt file split by space\n",
    "    with open(path , 'r' , encoding ='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    #separately store input and target text\n",
    "    for line in lines[:num_samples]:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        #now here the given dataset is french to english but i want eng to french trans so careful with the order\n",
    "        eng , fra = parts[0] , parts[1]\n",
    "        #add start and end for decoder\n",
    "        target_text = \"<start> \" + fra + \" <end>\"\n",
    "        input_texts.append(eng)\n",
    "        target_texts.append(target_text)\n",
    "        \n",
    "    return input_texts, target_texts\n",
    "\n",
    "input_texts, target_texts = load_data('fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3c60fd-371d-4bc0-a0fb-3fa3f26ed24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentences):\n",
    "    #convert in lowercase\n",
    "    sentences =  [s.lower() for s in sentences]\n",
    "    #remove everything except letters , digits , white spaces , and <>(remember the <start>)\n",
    "    #dont forget ^ this its the negation symbol i forgot and spend 30 minutes looking for bug\n",
    "    sentences = [re.sub(r\"[^a-zA-Z0-9<>\\s]\", \"\", s) for s in sentences]\n",
    "    return sentences\n",
    "input_texts = preprocess_text(input_texts)\n",
    "target_texts = preprocess_text(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5502d24e-cdc4-4e38-91d6-bec56be03d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize output\n",
    "#as we have already cleaned the text filter is off('' - do not remove anything)\n",
    "input_tokenizer = Tokenizer(filters = '')\n",
    "#go through all sentences and build a word-index vocab\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "#use the vocab and replce each word with its integer ID\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "#extract the word to index vocab or dict. for future uses\n",
    "input_word_index = input_tokenizer.word_index\n",
    "#find the max.sequence length\n",
    "max_input_len = max(len(seq) for seq in input_sequences)\n",
    "#pad with 0 if length is lower than max length\n",
    "encoder_input_data = pad_sequences(input_sequences , maxlen = max_input_len ,padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b478e039-0107-4211-9c82-18f346a4deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize output\n",
    "#No comments as everything is same as above\n",
    "target_tokenizer = Tokenizer(filters='')\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "target_word_index = target_tokenizer.word_index\n",
    "max_target_len = max(len(seq) for seq in target_sequences)\n",
    "decoder_input_data = pad_sequences(target_sequences, maxlen=max_target_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b51cb58-4d6b-420c-b40f-98b0921741bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating decoder targets\n",
    "\n",
    "#create a numpy array of same shape as decoder_input_data\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "#now we take all the columns from 1 to end from decoder_input_data and fill it in decoder_target_data from 0 to end - 1 \n",
    "#so basically we shifted left\n",
    "decoder_target_data[:,:-1] = decoder_input_data[:, 1:]\n",
    "#well this is actually not necessary but i still did that just to be safe , it does nothing but make sure last value is zero\n",
    "decoder_target_data[:, -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dac0263-f829-4f3a-96ad-8160d76036f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n",
      "4566\n"
     ]
    }
   ],
   "source": [
    "#embedding dimensions\n",
    "embedding_dim = 256 \n",
    "#LSTM hidden units\n",
    "lstm_units = 512\n",
    "#vocab size , +1 for padding token\n",
    "input_vocab_size = len(input_word_index)+1\n",
    "target_vocab_size = len(target_word_index)+1\n",
    "print(input_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4a4a3b4-381e-4f6e-bf06-9debdd717640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Model\n",
    "#encoder inputs\n",
    "encoder_inputs = Input(shape = (None , ))\n",
    "#embedding layer take encoder inputs of size vocab size as defined above and embed them and convert into dimensions of embedding_dim\n",
    "enc_emb = Embedding(input_vocab_size ,embedding_dim , mask_zero = True)(encoder_inputs)\n",
    "#a LSTM layer with hidden units = lstm_units and we want each hidden state as well as cell state so return_state is true and return_sequence is true\n",
    "encoder_outputs , state_h , state_c = LSTM(lstm_units, return_sequences = True, return_state = True)(enc_emb)\n",
    "#store the stats\n",
    "encoder_states = [state_h , state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f0db32e-3eb4-4e6d-a231-fb3d569b4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:965: UserWarning: Layer 'attention_concat_layer' (of type AttentionConcatLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Decoder Model\n",
    "decoder_inputs = Input(shape = (None , ))\n",
    "decoder_emb_layer = Embedding(target_vocab_size, embedding_dim, mask_zero=True, name='dec_emb')\n",
    "dec_emb = decoder_emb_layer(decoder_inputs)\n",
    "#just defining the LSTM layer and in addition to states we want output at each state/time step so return_sequence = True\n",
    "decoder_lstm = LSTM(lstm_units,return_sequences = True, return_state = True)\n",
    "#get the outputs by giving dec_emb as input and passing encoder's states as initial states\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = encoder_states)\n",
    "# Attention application (replace tf.map_fn/transpose with this)\n",
    "attention_concat = AttentionConcatLayer(lstm_units)\n",
    "decoder_combined_context = attention_concat(decoder_outputs, encoder_outputs)\n",
    "#final output dense layer applied to each time step (TimeDistributed)\n",
    "decoder_dense = TimeDistributed(Dense(target_vocab_size, activation='softmax'))\n",
    "decoder_outputs_final = decoder_dense(decoder_combined_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a72aab53-4285-4abc-95fc-24288972f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Model\n",
    "#combined model with teacher forcing\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_outputs_final)\n",
    "#we are using 'sparse_categorical_crossentropy' cause we didnt one-hot encode targets\n",
    "model.compile(optimizer = 'adam' , loss ='sparse_categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb280977-cf6b-49d3-9139-fe13cb9b8595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">517,632</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dec_emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168,896</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)] │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ dec_emb[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], │\n",
       "│                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)] │                 │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_concat_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,825</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionConcatLayer</span>)        │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ time_distributed              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4566</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,680,150</span> │ attention_concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)             │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m517,632\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dec_emb (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │       \u001b[38;5;34m1,168,896\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),       │       \u001b[38;5;34m1,574,912\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)] │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                 │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),       │       \u001b[38;5;34m1,574,912\u001b[0m │ dec_emb[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], │\n",
       "│                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)] │                 │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ attention_concat_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        │         \u001b[38;5;34m525,825\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mAttentionConcatLayer\u001b[0m)        │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ time_distributed              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4566\u001b[0m)        │       \u001b[38;5;34m4,680,150\u001b[0m │ attention_concat_layer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)             │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,042,327</span> (38.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,042,327\u001b[0m (38.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,042,327</span> (38.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,042,327\u001b[0m (38.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets check our model \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1feeb2e9-9017-4833-be2f-8a20afac2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 1s/step - accuracy: 0.6695 - loss: 3.4648 - val_accuracy: 0.7562 - val_loss: 1.7946\n",
      "Epoch 2/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 1s/step - accuracy: 0.7971 - loss: 1.4077 - val_accuracy: 0.7796 - val_loss: 1.5826\n",
      "Epoch 3/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.8176 - loss: 1.1499 - val_accuracy: 0.7980 - val_loss: 1.4980\n",
      "Epoch 4/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 1s/step - accuracy: 0.8328 - loss: 0.9685 - val_accuracy: 0.8058 - val_loss: 1.4600\n",
      "Epoch 5/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.8473 - loss: 0.8027 - val_accuracy: 0.8135 - val_loss: 1.4485\n",
      "Epoch 6/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.8604 - loss: 0.6604 - val_accuracy: 0.8163 - val_loss: 1.4391\n",
      "Epoch 7/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.8753 - loss: 0.5411 - val_accuracy: 0.8187 - val_loss: 1.4329\n",
      "Epoch 8/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.8883 - loss: 0.4547 - val_accuracy: 0.8192 - val_loss: 1.4403\n",
      "Epoch 9/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.9018 - loss: 0.3777 - val_accuracy: 0.8227 - val_loss: 1.4343\n",
      "Epoch 10/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.9113 - loss: 0.3244 - val_accuracy: 0.8230 - val_loss: 1.4279\n",
      "Epoch 11/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.9189 - loss: 0.2814 - val_accuracy: 0.8259 - val_loss: 1.4251\n",
      "Epoch 12/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 1s/step - accuracy: 0.9273 - loss: 0.2425 - val_accuracy: 0.8257 - val_loss: 1.4385\n",
      "Epoch 13/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 1s/step - accuracy: 0.9343 - loss: 0.2142 - val_accuracy: 0.8272 - val_loss: 1.4249\n",
      "Epoch 14/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.9370 - loss: 0.1948 - val_accuracy: 0.8274 - val_loss: 1.4311\n",
      "Epoch 15/15\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - accuracy: 0.9396 - loss: 0.1810 - val_accuracy: 0.8255 - val_loss: 1.4434\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "#... maens all the dimensions\n",
    "history = model.fit([encoder_input_data,decoder_input_data], decoder_target_data[...,np.newaxis], \n",
    "                    batch_size = 64, epochs = 15,\n",
    "                   validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aff3371f-11e0-4631-adb4-9998ff5b81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference encoder model- just takes input and gives out hidden states\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "#Decoder setup\n",
    "decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "decoder_hidden_state_input = Input(shape=(max_input_len, lstm_units))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "#single step decoder input token (1 timestep)\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "dec_emb2 = decoder_emb_layer(decoder_inputs_single)\n",
    "#run the decoder LSTM for a single timestep with passed states\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "\n",
    "#compute attention context vector based on decoder output and encoder hidden states\n",
    "attention_layer = BahdanauAttention(lstm_units)\n",
    "context_vector2, attention_weights2 = attention_layer(decoder_outputs2[:, 0, :], decoder_hidden_state_input)\n",
    "#expand dims for concatenation to match decoder_outputs2 shape\n",
    "context_vector2 = Lambda(lambda x: tf.expand_dims(x, 1))(context_vector2)\n",
    "decoder_combined_context2 = Concatenate(axis=-1)([decoder_outputs2, context_vector2])\n",
    "\n",
    "#final output dense layer for next word prediction\n",
    "decoder_outputs2 = decoder_dense(decoder_combined_context2)\n",
    "\n",
    "#final inference decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single, decoder_hidden_state_input] + decoder_states_inputs,\n",
    "    [decoder_outputs2, state_h2, state_c2, attention_weights2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67929703-c2f0-47fc-9fc1-2b91968eebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_index = {i: word for word, i in target_tokenizer.word_index.items()}\n",
    "reverse_target_index[0] = ''\n",
    "target_index_word = target_tokenizer.word_index\n",
    "start_token = target_index_word['<start>']\n",
    "end_token = target_index_word['<end>']\n",
    "\n",
    "def decode_sequence_with_attention(input_seq):\n",
    "    enc_outs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = start_token\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    attention_plot = []\n",
    "\n",
    "    while not stop_condition:\n",
    "        # The decoder_model must output attention weights as the last return value\n",
    "        output_tokens, h, c, attention_weights = decoder_model.predict([target_seq, enc_outs, state_h, state_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_index.get(sampled_token_index, '')\n",
    "\n",
    "        attention_plot.append(attention_weights[0].squeeze())\n",
    "\n",
    "        if (sampled_word == '<end>' or len(decoded_sentence) > max_target_len):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence.append(sampled_word)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    attention_plot = np.array(attention_plot)\n",
    "    return ' '.join(decoded_sentence), attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e5a7ca1-535c-4d42-a2f2-56268705a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attention, input_sentence, predicted_sentence):\n",
    "    input_tokens = input_sentence.strip().split()\n",
    "    output_tokens = predicted_sentence.strip().split()\n",
    "    fig, ax = plt.subplots(figsize=(min(16, len(input_tokens)), min(12, len(output_tokens))))\n",
    "    heatmap = attention[:len(output_tokens), :len(input_tokens)]\n",
    "    im = ax.imshow(heatmap, cmap='viridis')\n",
    "    ax.set_xticks(range(len(input_tokens)))\n",
    "    ax.set_yticks(range(len(output_tokens)))\n",
    "    ax.set_xticklabels(input_tokens, rotation=90)\n",
    "    ax.set_yticklabels(output_tokens)\n",
    "    plt.xlabel(\"Input Sequence\")\n",
    "    plt.ylabel(\"Output Sequence\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f62046e-a8f1-495f-b7f4-6bd4f6a6b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m encoder_input_data[idx:idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m input_sentence \u001b[38;5;241m=\u001b[39m input_texts[idx]\n\u001b[1;32m----> 5\u001b[0m predicted_sentence, attention \u001b[38;5;241m=\u001b[39m decode_sequence_with_attention(input_seq)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_sentence)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_sentence)\n",
      "Cell \u001b[1;32mIn[50], line 18\u001b[0m, in \u001b[0;36mdecode_sequence_with_attention\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_plot \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# The decoder_model must output attention weights as the last return value\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     output_tokens, h, c, attention_weights \u001b[38;5;241m=\u001b[39m decoder_model\u001b[38;5;241m.\u001b[39mpredict([target_seq, enc_outs, state_h, state_c])\n\u001b[0;32m     19\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     20\u001b[0m     sampled_word \u001b[38;5;241m=\u001b[39m reverse_target_index\u001b[38;5;241m.\u001b[39mget(sampled_token_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)"
     ]
    }
   ],
   "source": [
    "#translate and show the attention heatmap for the first sentence\n",
    "idx = 0\n",
    "input_seq = encoder_input_data[idx:idx+1]\n",
    "input_sentence = input_texts[idx]\n",
    "predicted_sentence, attention = decode_sequence_with_attention(input_seq)\n",
    "print(\"Input:\", input_sentence)\n",
    "print(\"Predicted:\", predicted_sentence)\n",
    "plot_attention_heatmap(attention, input_sentence, predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "034b69de-0e8d-4067-81a2-73c3dd29034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget:\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_texts[index])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded)\n\u001b[1;32m---> 10\u001b[0m predict_sample(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 4\u001b[0m, in \u001b[0;36mpredict_sample\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_sample\u001b[39m(index):\n\u001b[0;32m      3\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m encoder_input_data[index:index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m decode_sequence_with_attention(input_seq)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_texts[index])\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget:\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_texts[index])\n",
      "Cell \u001b[1;32mIn[50], line 18\u001b[0m, in \u001b[0;36mdecode_sequence_with_attention\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_plot \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# The decoder_model must output attention weights as the last return value\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     output_tokens, h, c, attention_weights \u001b[38;5;241m=\u001b[39m decoder_model\u001b[38;5;241m.\u001b[39mpredict([target_seq, enc_outs, state_h, state_c])\n\u001b[0;32m     19\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     20\u001b[0m     sampled_word \u001b[38;5;241m=\u001b[39m reverse_target_index\u001b[38;5;241m.\u001b[39mget(sampled_token_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)"
     ]
    }
   ],
   "source": [
    "#lets check\n",
    "def predict_sample(index):\n",
    "    input_seq = encoder_input_data[index:index+1]\n",
    "    decoded = decode_sequence_with_attention(input_seq)\n",
    "    print(\"Input:\", input_texts[index])\n",
    "    print(\"Target:\", target_texts[index])\n",
    "    print(\"Predicted:\", decoded)\n",
    "\n",
    "\n",
    "predict_sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c73c28d-d49b-4632-8c2b-124e0a51a05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# predict 5 samples from index 0\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m predict_samples(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[63], line 5\u001b[0m, in \u001b[0;36mpredict_samples\u001b[1;34m(start_index, num_samples)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_index, start_index \u001b[38;5;241m+\u001b[39m num_samples):\n\u001b[0;32m      4\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m encoder_input_data[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m decode_sequence_with_attention(input_seq)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;124m\"\u001b[39m, input_texts[i])\n",
      "Cell \u001b[1;32mIn[50], line 18\u001b[0m, in \u001b[0;36mdecode_sequence_with_attention\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_plot \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# The decoder_model must output attention weights as the last return value\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     output_tokens, h, c, attention_weights \u001b[38;5;241m=\u001b[39m decoder_model\u001b[38;5;241m.\u001b[39mpredict([target_seq, enc_outs, state_h, state_c])\n\u001b[0;32m     19\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     20\u001b[0m     sampled_word \u001b[38;5;241m=\u001b[39m reverse_target_index\u001b[38;5;241m.\u001b[39mget(sampled_token_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)"
     ]
    }
   ],
   "source": [
    "# lets translate and check 5 sentences\n",
    "def predict_samples(start_index, num_samples=5):\n",
    "    for i in range(start_index, start_index + num_samples):\n",
    "        input_seq = encoder_input_data[i:i+1]\n",
    "        decoded = decode_sequence_with_attention(input_seq)\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(\"Input: \", input_texts[i])\n",
    "        print(\"Target:\", target_texts[i])\n",
    "        print(\"Predicted:\", decoded)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# predict 5 samples from index 0\n",
    "predict_samples(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3807409-d408-4504-a949-1d71d1418c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80645e58-d06b-43e1-87ef-c162b0239bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu_score(num_samples=100):\n",
    "    total_score = 0.0\n",
    "    individual_scores = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_seq = encoder_input_data[i:i+1]\n",
    "        decoded_sentence = decode_sequence_with_attention(input_seq).strip().split()\n",
    "        reference_sentence = target_texts[i].replace('<start>', '').replace('<end>', '').strip().split()\n",
    "\n",
    "        score = sentence_bleu([reference_sentence], decoded_sentence, smoothing_function=smoothie)\n",
    "        individual_scores.append(score)\n",
    "        total_score += score\n",
    "\n",
    "    avg_bleu = total_score / num_samples\n",
    "    print(f\"\\nAverage BLEU score on {num_samples} samples: {avg_bleu:.4f}\")\n",
    "    return avg_bleu, individual_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13e4be4e-d4c9-4210-8258-fc5eefcda73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_bleu, scores \u001b[38;5;241m=\u001b[39m evaluate_bleu_score(\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[71], line 7\u001b[0m, in \u001b[0;36mevaluate_bleu_score\u001b[1;34m(num_samples)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m      6\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m encoder_input_data[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m     decoded_sentence \u001b[38;5;241m=\u001b[39m decode_sequence_with_attention(input_seq)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      8\u001b[0m     reference_sentence \u001b[38;5;241m=\u001b[39m target_texts[i]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     10\u001b[0m     score \u001b[38;5;241m=\u001b[39m sentence_bleu([reference_sentence], decoded_sentence, smoothing_function\u001b[38;5;241m=\u001b[39msmoothie)\n",
      "Cell \u001b[1;32mIn[50], line 18\u001b[0m, in \u001b[0;36mdecode_sequence_with_attention\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     14\u001b[0m attention_plot \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# The decoder_model must output attention weights as the last return value\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     output_tokens, h, c, attention_weights \u001b[38;5;241m=\u001b[39m decoder_model\u001b[38;5;241m.\u001b[39mpredict([target_seq, enc_outs, state_h, state_c])\n\u001b[0;32m     19\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     20\u001b[0m     sampled_word \u001b[38;5;241m=\u001b[39m reverse_target_index\u001b[38;5;241m.\u001b[39mget(sampled_token_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling BroadcastTo.call().\n\n\u001b[1mFailed to convert elements of (None, 1, 512) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by BroadcastTo.call():\n  • x=tf.Tensor(shape=(1, 1, 1), dtype=bool)"
     ]
    }
   ],
   "source": [
    "avg_bleu, scores = evaluate_bleu_score(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25180afc-414a-46ba-a3bd-6fef1ae13841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560c49e-9d93-47b4-a17a-e74c15d91528",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(\"Accuracy and Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248718bf-15a8-4deb-9593-a38391260673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb411ac-8efc-484f-b26e-96ecb2212b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf88542-1d5c-43d6-888a-2d8dea036942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274ba19-7255-40ce-b2a7-d25eddf1ae33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805d353-cad1-4e51-931c-418893da37e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88feabe1-a12c-4b8f-a89f-1de19b22b439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
